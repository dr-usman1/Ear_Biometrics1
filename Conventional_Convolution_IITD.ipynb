{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr\n",
    "from zipfile import ZipFile\n",
    "from random import sample\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os import path\n",
    "import h5py\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = IITD_Dataset.7z\n",
    "filename = 'IITD_Dataset.7z'\n",
    "# if (path.exists(filename)):\n",
    "#     !rm $filename\n",
    "#     print(\"File Removed!\")\n",
    "# print(\"Downloading Dataset...\")\n",
    "# wget.download(data_path, filename)\n",
    "# print(\"Download Complete!\")\n",
    "\n",
    "with py7zr.SevenZipFile(filename, mode='r') as z:\n",
    "    z.extractall()\n",
    "    print(\"Extracted Dataset!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'ear/processed/221'\n",
    "images_name = listdir(src_dir)\n",
    "images_name_temp = []\n",
    "subject_id = []\n",
    "for img_ind in range(0, len(images_name)):\n",
    "    if(not(images_name[img_ind]=='Thumbs.db')):\n",
    "        subject_id.append(int(images_name[img_ind].split('_')[0]))\n",
    "        images_name_temp.append(images_name[img_ind])\n",
    "        \n",
    "\n",
    "images_name = images_name_temp\n",
    "images_name_ordered = []\n",
    "subject_id_ordered = []\n",
    "\n",
    "sub_ind = sorted(range(len(subject_id)), key=subject_id.__getitem__)   # Sorting the subject id\n",
    "for pos, item in enumerate(sub_ind):\n",
    "    images_name_ordered.append(images_name[item])\n",
    "    subject_id_ordered.append(subject_id[item])\n",
    "print(sub_ind)\n",
    "images_name = images_name_ordered\n",
    "subject_id = subject_id_ordered\n",
    "\n",
    "# print(subject_id)\n",
    "# print(images_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ind = 0\n",
    "ear_images = []\n",
    "sub_labels = []\n",
    "target_size = (180, 50)\n",
    "\n",
    "for sub_ind in range (0,len(subject_id)):\n",
    "    img_path = src_dir + '/' + images_name[sub_ind]\n",
    "    ear_img = (plt.imread(img_path))/255\n",
    "\n",
    "    ear_img = Image.open(img_path)\n",
    "    ear_img = ear_img.resize(target_size, Image.ANTIALIAS)\n",
    "    ear_img = np.asarray(ear_img).astype(np.float32)/255\n",
    "\n",
    "    ear_images.append(ear_img)\n",
    "    sub_labels.append(subject_id[sub_ind]-1)\n",
    "\n",
    "ear_images = np.array(ear_images)\n",
    "sub_labels = np.array(sub_labels)\n",
    "print('Sub_Labels = ', sub_labels)\n",
    "print(\"Shape of the dataset:\", ear_images.shape)\n",
    "print(\"Shape of the labels:\", sub_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ear_images, sub_labels, test_size=0.382093316519, random_state=42)\n",
    "\n",
    "print(\"Shape of the training dataset:\", X_train.shape)  \n",
    "print(\"Shape of the training labels:\", y_train.shape)   \n",
    "print(\"Shape of the testing dataset:\", X_test.shape)    \n",
    "print(\"Shape of the testing labels:\", y_test.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "# import torchsummary \n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pytorch_BUS_Final_Model_C1(torch.nn.Module):\n",
    "    def __init__(self, num_classes=221, num_filters=8, input_shape=(180,50,1)):\n",
    "        super(Pytorch_BUS_Final_Model_C1, self).__init__()\n",
    "        kernel_size = 3\n",
    "        #Encoder Layer 1\n",
    "        self.encoder_layer1_name = 'encoder_layer1'\n",
    "        self.encoder_layer1_conv = torch.nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, padding='same')\n",
    "        self.encoder_layer1_activation = torch.nn.ReLU()\n",
    "        self.encoder_layer1_pool = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        #Encoder Layer 2\n",
    "        self.encoder_layer2_name = 'encoder_layer2'\n",
    "        self.encoder_layer2_conv = torch.nn.Conv2d(in_channels=num_filters, out_channels=num_filters*2, kernel_size=kernel_size, padding='same')\n",
    "        self.encoder_layer2_activation = torch.nn.ReLU()\n",
    "        self.encoder_layer2_pool = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.encoder_layer2_batchnorm = torch.nn.BatchNorm2d(num_features=num_filters*2,eps=1e-3,momentum=0.99) \n",
    "\n",
    "        #Encoder Layer 3\n",
    "        self.encoder_layer3_name = 'encoder_layer3'   \n",
    "        self.encoder_layer3_conv = torch.nn.Conv2d(in_channels=num_filters*2, out_channels=num_filters*4, kernel_size=kernel_size, padding='same')  \n",
    "        self.encoder_layer3_activation = torch.nn.ReLU()\n",
    "        self.encoder_layer3_pool = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        #self.encoder_layer3_batchnorm = torch.nn.BatchNorm2d(num_features=num_filters*4,eps=1e-3,momentum=0.99)\n",
    "\n",
    "        #Encoder Layer 4\n",
    "        self.encoder_layer4_name = 'encoder_layer4'\n",
    "        self.encoder_layer4_conv = torch.nn.Conv2d(in_channels=num_filters*4, out_channels=num_filters*8, kernel_size=kernel_size, padding='same')\n",
    "        self.encoder_layer4_activation = torch.nn.ReLU()\n",
    "        self.encoder_layer4_pool = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.encoder_layer4_batchnorm = torch.nn.BatchNorm2d(num_features=num_filters*8,eps=1e-3,momentum=0.99)\n",
    "\n",
    "        #Encoder Layer 5\n",
    "        self.encoder_layer5_name = 'encoder_layer5'\n",
    "        self.encoder_layer5_conv = torch.nn.Conv2d(in_channels=num_filters*8, out_channels=num_filters*16, kernel_size=kernel_size, padding='same')\n",
    "        self.encoder_layer5_activation = torch.nn.ReLU()\n",
    "        self.encoder_layer5_pool = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        # self.encoder_layer5_batchnorm = torch.nn.BatchNorm2d(num_features=num_filters*16,eps=1e-3,momentum=0.99)\n",
    "\n",
    "        #Encoder Layer 6\n",
    "        self.encoder_layer6_name = 'encoder_layer6'\n",
    "        self.encoder_layer6_conv = torch.nn.Conv2d(in_channels=num_filters*16, out_channels=num_filters*32, kernel_size=kernel_size, padding='same')\n",
    "        self.encoder_layer6_activation = torch.nn.ReLU()\n",
    "        #self.encoder_layer6_pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.encoder_layer6_batchnorm = torch.nn.BatchNorm2d(num_features=num_filters*32,eps=1e-3,momentum=0.99)\n",
    "        \n",
    "        #Dense Layers\n",
    "        self.fc1_flatten = torch.nn.Flatten()\n",
    "        self.fc1_linear = torch.nn.Linear(32*num_filters*(input_shape[0]//(2**5))*(input_shape[1]//(2**5)), out_features= num_classes)\n",
    "        self.fc1_activation = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self,x):\n",
    "        #Encoder Layer 1\n",
    "        out = self.encoder_layer1_conv(x)\n",
    "        out = self.encoder_layer1_activation(out)   \n",
    "        out = self.encoder_layer1_pool(out)\n",
    "\n",
    "        #Encoder Layer 2\n",
    "        out = self.encoder_layer2_conv(out)\n",
    "        out = self.encoder_layer2_activation(out)\n",
    "        out = self.encoder_layer2_pool(out)\n",
    "        out = self.encoder_layer2_batchnorm(out)\n",
    "\n",
    "        #Encoder Layer 3\n",
    "        out = self.encoder_layer3_conv(out)\n",
    "        out = self.encoder_layer3_activation(out)\n",
    "        out = self.encoder_layer3_pool(out)\n",
    "        #out = self.encoder_layer3_batchnorm(out)\n",
    "\n",
    "        #Encoder Layer 4\n",
    "        out = self.encoder_layer4_conv(out)\n",
    "        out = self.encoder_layer4_activation(out)\n",
    "        out = self.encoder_layer4_pool(out)\n",
    "        out = self.encoder_layer4_batchnorm(out)\n",
    "\n",
    "        #Encoder Layer 5\n",
    "        out = self.encoder_layer5_conv(out)\n",
    "        out = self.encoder_layer5_activation(out)\n",
    "        out = self.encoder_layer5_pool(out)\n",
    "        # out = self.encoder_layer5_batchnorm(out)\n",
    "\n",
    "        #Encoder Layer 6\n",
    "        out = self.encoder_layer6_conv(out)\n",
    "        out = self.encoder_layer6_activation(out)\n",
    "        #out = self.encoder_layer6_pool(out)\n",
    "        out = self.encoder_layer6_batchnorm(out)\n",
    "\n",
    "        #Dense Layers\n",
    "        out = self.fc1_flatten(out)\n",
    "        out = self.fc1_linear(out)\n",
    "        out = self.fc1_activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = Pytorch_BUS_Final_Model_C1()\n",
    "print(pytorch_model)\n",
    "summary(pytorch_model, input_size=(1,1, 180, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = torch.tensor(X_train[0].reshape(1,1,180,50),device='cuda').float()\n",
    "print (input_x.shape)\n",
    "output = pytorch_model(input_x)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loader = torch.utils.data.DataLoader(torch.tensor(X_train),  batch_size=32, shuffle=True)\n",
    "# validation_loader = torch.utils.data.DataLoader(X_test,  batch_size=32, shuffle=True)\n",
    "training_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=32, pin_memory='True', pin_memory_device='cuda', shuffle=True)\n",
    "validation_loader = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)), batch_size=32, pin_memory='True', pin_memory_device='cuda')\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "pytorch_model = Pytorch_BUS_Final_Model_C1().to(torch.device('cuda'))\n",
    "\n",
    "def training_one_epoch():\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    valid_loss = 0\n",
    "    valid_correct = 0\n",
    "\n",
    "    pytorch_model.train(True)\n",
    "    for i, data in enumerate(training_loader,0):\n",
    "        train_input, train_label = data\n",
    "        train_input = train_input.unsqueeze(1).float().to(torch.device('cuda'))\n",
    "        train_label = torch.tensor(to_categorical(train_label, 221)).float().to(torch.device('cuda'))\n",
    "\n",
    "        if len(train_label.shape) == 1:\n",
    "            train_label = train_label.unsqueeze(0)\n",
    "\n",
    "        train_input = train_input.to(torch.device('cuda'))\n",
    "        train_label = train_label.to(torch.device('cuda'))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_output = pytorch_model(train_input)\n",
    "\n",
    "        loss = loss_fn(train_output, train_label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        for batch_count in range(train_label.shape[0]):\n",
    "            if torch.argmax(train_output[batch_count,:]) == torch.argmax(train_label[batch_count]):\n",
    "                train_correct += 1\n",
    "\n",
    "    pytorch_model.train(False)\n",
    "    for i, data in enumerate(validation_loader,0):\n",
    "        valid_input, valid_label = data\n",
    "        valid_input = valid_input.unsqueeze(1).float().to(torch.device('cuda'))\n",
    "        valid_label = torch.tensor(to_categorical(valid_label, 221)).float().to(torch.device('cuda'))\n",
    "\n",
    "        if len(valid_label.shape) == 1:\n",
    "            valid_label = valid_label.unsqueeze(0)\n",
    "\n",
    "        valid_input = valid_input.to(torch.device('cuda'))\n",
    "        valid_label = valid_label.to(torch.device('cuda'))\n",
    "\n",
    "        valid_output = pytorch_model(valid_input)\n",
    "\n",
    "        loss = loss_fn(valid_output, valid_label)\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "        for batch_count in range(valid_label.shape[0]):\n",
    "            if torch.argmax(valid_output[batch_count,:]) == torch.argmax(valid_label[batch_count]):\n",
    "                valid_correct += 1\n",
    "    \n",
    "    print(f\"Training: \\n Training Accuracy: {100*train_correct/len(training_loader.dataset)}%, Average Training Loss: {train_loss/len(training_loader)}\")\n",
    "\n",
    "    print(f\"Validation: \\n Validation Accuracy: {100*valid_correct/len(validation_loader.dataset)}%, Average Validation Loss: {valid_loss/len(validation_loader)}\")\n",
    "    return train_loss, valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\209141\\AppData\\Local\\Temp\\ipykernel_31660\\1924088813.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.fc1_activation(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      " Training Accuracy: 1.4285714285714286%, Average Training Loss: 5.394372642040253\n",
      "Validation: \n",
      " Validation Accuracy: 3.6303630363036303%, Average Validation Loss: 5.396330547332764\n",
      "Epoch 2:\n",
      "Training: \n",
      " Training Accuracy: 4.8979591836734695%, Average Training Loss: 5.386608302593231\n",
      "Validation: \n",
      " Validation Accuracy: 1.3201320132013201%, Average Validation Loss: 5.395488595962524\n",
      "Epoch 3:\n",
      "Training: \n",
      " Training Accuracy: 5.1020408163265305%, Average Training Loss: 5.37213659286499\n",
      "Validation: \n",
      " Validation Accuracy: 1.6501650165016502%, Average Validation Loss: 5.3900840282440186\n",
      "Epoch 4:\n",
      "Training: \n",
      " Training Accuracy: 7.959183673469388%, Average Training Loss: 5.347459554672241\n",
      "Validation: \n",
      " Validation Accuracy: 3.6303630363036303%, Average Validation Loss: 5.374849939346314\n",
      "Epoch 5:\n",
      "Training: \n",
      " Training Accuracy: 13.46938775510204%, Average Training Loss: 5.307069778442383\n",
      "Validation: \n",
      " Validation Accuracy: 4.9504950495049505%, Average Validation Loss: 5.364626407623291\n",
      "Epoch 6:\n",
      "Training: \n",
      " Training Accuracy: 17.142857142857142%, Average Training Loss: 5.279272377490997\n",
      "Validation: \n",
      " Validation Accuracy: 6.270627062706271%, Average Validation Loss: 5.352104806900025\n",
      "Epoch 7:\n",
      "Training: \n",
      " Training Accuracy: 20.816326530612244%, Average Training Loss: 5.230696618556976\n",
      "Validation: \n",
      " Validation Accuracy: 7.260726072607261%, Average Validation Loss: 5.352501964569091\n",
      "Epoch 8:\n",
      "Training: \n",
      " Training Accuracy: 27.959183673469386%, Average Training Loss: 5.199965566396713\n",
      "Validation: \n",
      " Validation Accuracy: 9.24092409240924%, Average Validation Loss: 5.32786865234375\n",
      "Epoch 9:\n",
      "Training: \n",
      " Training Accuracy: 35.10204081632653%, Average Training Loss: 5.142273306846619\n",
      "Validation: \n",
      " Validation Accuracy: 12.541254125412541%, Average Validation Loss: 5.318711471557617\n",
      "Epoch 10:\n",
      "Training: \n",
      " Training Accuracy: 42.44897959183673%, Average Training Loss: 5.080534726381302\n",
      "Validation: \n",
      " Validation Accuracy: 16.17161716171617%, Average Validation Loss: 5.304754638671875\n",
      "Epoch 11:\n",
      "Training: \n",
      " Training Accuracy: 48.775510204081634%, Average Training Loss: 5.019822984933853\n",
      "Validation: \n",
      " Validation Accuracy: 20.462046204620464%, Average Validation Loss: 5.276523590087891\n",
      "Epoch 12:\n",
      "Training: \n",
      " Training Accuracy: 52.857142857142854%, Average Training Loss: 4.971997022628784\n",
      "Validation: \n",
      " Validation Accuracy: 22.77227722772277%, Average Validation Loss: 5.241371965408325\n",
      "Epoch 13:\n",
      "Training: \n",
      " Training Accuracy: 58.36734693877551%, Average Training Loss: 4.919544011354446\n",
      "Validation: \n",
      " Validation Accuracy: 24.752475247524753%, Average Validation Loss: 5.223750734329224\n",
      "Epoch 14:\n",
      "Training: \n",
      " Training Accuracy: 62.44897959183673%, Average Training Loss: 4.860039323568344\n",
      "Validation: \n",
      " Validation Accuracy: 28.712871287128714%, Average Validation Loss: 5.2230713844299315\n",
      "Epoch 15:\n",
      "Training: \n",
      " Training Accuracy: 68.16326530612245%, Average Training Loss: 4.813368946313858\n",
      "Validation: \n",
      " Validation Accuracy: 31.683168316831683%, Average Validation Loss: 5.174609184265137\n",
      "Epoch 16:\n",
      "Training: \n",
      " Training Accuracy: 73.87755102040816%, Average Training Loss: 4.768601715564728\n",
      "Validation: \n",
      " Validation Accuracy: 32.67326732673267%, Average Validation Loss: 5.221078205108642\n",
      "Epoch 17:\n",
      "Training: \n",
      " Training Accuracy: 77.34693877551021%, Average Training Loss: 4.71416249871254\n",
      "Validation: \n",
      " Validation Accuracy: 38.943894389438945%, Average Validation Loss: 5.13922872543335\n",
      "Epoch 18:\n",
      "Training: \n",
      " Training Accuracy: 81.0204081632653%, Average Training Loss: 4.674029767513275\n",
      "Validation: \n",
      " Validation Accuracy: 41.254125412541256%, Average Validation Loss: 5.1568012714385985\n",
      "Epoch 19:\n",
      "Training: \n",
      " Training Accuracy: 83.46938775510205%, Average Training Loss: 4.641840994358063\n",
      "Validation: \n",
      " Validation Accuracy: 45.87458745874587%, Average Validation Loss: 5.075455951690674\n",
      "Epoch 20:\n",
      "Training: \n",
      " Training Accuracy: 85.51020408163265%, Average Training Loss: 4.6040318608284\n",
      "Validation: \n",
      " Validation Accuracy: 48.84488448844885%, Average Validation Loss: 5.051830053329468\n",
      "Epoch 21:\n",
      "Training: \n",
      " Training Accuracy: 87.75510204081633%, Average Training Loss: 4.580425500869751\n",
      "Validation: \n",
      " Validation Accuracy: 52.805280528052805%, Average Validation Loss: 5.046791362762451\n",
      "Epoch 22:\n",
      "Training: \n",
      " Training Accuracy: 89.59183673469387%, Average Training Loss: 4.551891177892685\n",
      "Validation: \n",
      " Validation Accuracy: 55.445544554455445%, Average Validation Loss: 5.032896327972412\n",
      "Epoch 23:\n",
      "Training: \n",
      " Training Accuracy: 91.42857142857143%, Average Training Loss: 4.527026951313019\n",
      "Validation: \n",
      " Validation Accuracy: 58.085808580858085%, Average Validation Loss: 5.010939025878907\n",
      "Epoch 24:\n",
      "Training: \n",
      " Training Accuracy: 93.26530612244898%, Average Training Loss: 4.513188660144806\n",
      "Validation: \n",
      " Validation Accuracy: 57.42574257425743%, Average Validation Loss: 5.003865575790405\n",
      "Epoch 25:\n",
      "Training: \n",
      " Training Accuracy: 94.48979591836735%, Average Training Loss: 4.499859720468521\n",
      "Validation: \n",
      " Validation Accuracy: 59.40594059405941%, Average Validation Loss: 4.975074005126953\n",
      "Epoch 26:\n",
      "Training: \n",
      " Training Accuracy: 95.10204081632654%, Average Training Loss: 4.483995288610458\n",
      "Validation: \n",
      " Validation Accuracy: 58.745874587458744%, Average Validation Loss: 4.999166059494018\n",
      "Epoch 27:\n",
      "Training: \n",
      " Training Accuracy: 95.91836734693878%, Average Training Loss: 4.474640280008316\n",
      "Validation: \n",
      " Validation Accuracy: 64.35643564356435%, Average Validation Loss: 4.932318162918091\n",
      "Epoch 28:\n",
      "Training: \n",
      " Training Accuracy: 96.3265306122449%, Average Training Loss: 4.465326935052872\n",
      "Validation: \n",
      " Validation Accuracy: 63.366336633663366%, Average Validation Loss: 4.920401239395142\n",
      "Epoch 29:\n",
      "Training: \n",
      " Training Accuracy: 96.93877551020408%, Average Training Loss: 4.454144328832626\n",
      "Validation: \n",
      " Validation Accuracy: 66.66666666666667%, Average Validation Loss: 4.93737096786499\n",
      "Epoch 30:\n",
      "Training: \n",
      " Training Accuracy: 97.75510204081633%, Average Training Loss: 4.448161065578461\n",
      "Validation: \n",
      " Validation Accuracy: 63.03630363036304%, Average Validation Loss: 4.956074333190918\n",
      "Epoch 31:\n",
      "Training: \n",
      " Training Accuracy: 97.75510204081633%, Average Training Loss: 4.439500421285629\n",
      "Validation: \n",
      " Validation Accuracy: 65.67656765676567%, Average Validation Loss: 4.9344957828521725\n",
      "Epoch 32:\n",
      "Training: \n",
      " Training Accuracy: 98.57142857142857%, Average Training Loss: 4.441244721412659\n",
      "Validation: \n",
      " Validation Accuracy: 65.67656765676567%, Average Validation Loss: 4.893874406814575\n",
      "Epoch 33:\n",
      "Training: \n",
      " Training Accuracy: 98.57142857142857%, Average Training Loss: 4.430729568004608\n",
      "Validation: \n",
      " Validation Accuracy: 66.996699669967%, Average Validation Loss: 4.898541879653931\n",
      "Epoch 34:\n",
      "Training: \n",
      " Training Accuracy: 99.18367346938776%, Average Training Loss: 4.42764151096344\n",
      "Validation: \n",
      " Validation Accuracy: 65.67656765676567%, Average Validation Loss: 4.925717401504516\n",
      "Epoch 35:\n",
      "Training: \n",
      " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.426286399364471\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.884094285964966\n",
      "Epoch 36:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.418845474720001\n",
      "Validation: \n",
      " Validation Accuracy: 66.66666666666667%, Average Validation Loss: 4.888123083114624\n",
      "Epoch 37:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.418710887432098\n",
      "Validation: \n",
      " Validation Accuracy: 67.65676567656766%, Average Validation Loss: 4.914502191543579\n",
      "Epoch 38:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4163157641887665\n",
      "Validation: \n",
      " Validation Accuracy: 67.65676567656766%, Average Validation Loss: 4.875394058227539\n",
      "Epoch 39:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.415995270013809\n",
      "Validation: \n",
      " Validation Accuracy: 66.66666666666667%, Average Validation Loss: 4.887239027023315\n",
      "Epoch 40:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4150815308094025\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.883926057815552\n",
      "Epoch 41:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.414424747228622\n",
      "Validation: \n",
      " Validation Accuracy: 68.64686468646865%, Average Validation Loss: 4.8829991817474365\n",
      "Epoch 42:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.414763957262039\n",
      "Validation: \n",
      " Validation Accuracy: 65.67656765676567%, Average Validation Loss: 4.881551694869995\n",
      "Epoch 43:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.414528548717499\n",
      "Validation: \n",
      " Validation Accuracy: 66.33663366336634%, Average Validation Loss: 4.879401636123657\n",
      "Epoch 44:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.418212860822678\n",
      "Validation: \n",
      " Validation Accuracy: 69.63696369636963%, Average Validation Loss: 4.861193561553955\n",
      "Epoch 45:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.415377169847488\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.873721599578857\n",
      "Epoch 46:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.413660883903503\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.877061462402343\n",
      "Epoch 47:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.413285702466965\n",
      "Validation: \n",
      " Validation Accuracy: 66.33663366336634%, Average Validation Loss: 4.8646149158477785\n",
      "Epoch 48:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.41325917840004\n",
      "Validation: \n",
      " Validation Accuracy: 65.01650165016501%, Average Validation Loss: 4.8933045864105225\n",
      "Epoch 49:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.413171887397766\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.875573778152466\n",
      "Epoch 50:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412984937429428\n",
      "Validation: \n",
      " Validation Accuracy: 67.32673267326733%, Average Validation Loss: 4.854649829864502\n",
      "Epoch 51:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412622809410095\n",
      "Validation: \n",
      " Validation Accuracy: 66.66666666666667%, Average Validation Loss: 4.852602910995484\n",
      "Epoch 52:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412617444992065\n",
      "Validation: \n",
      " Validation Accuracy: 67.32673267326733%, Average Validation Loss: 4.86237735748291\n",
      "Epoch 53:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4128477871418\n",
      "Validation: \n",
      " Validation Accuracy: 66.996699669967%, Average Validation Loss: 4.8457359790802\n",
      "Epoch 54:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412472069263458\n",
      "Validation: \n",
      " Validation Accuracy: 67.65676567656766%, Average Validation Loss: 4.8523153305053714\n",
      "Epoch 55:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412288129329681\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.850948333740234\n",
      "Epoch 56:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412486493587494\n",
      "Validation: \n",
      " Validation Accuracy: 66.66666666666667%, Average Validation Loss: 4.853353452682495\n",
      "Epoch 57:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412131130695343\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.846924257278443\n",
      "Epoch 58:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412569671869278\n",
      "Validation: \n",
      " Validation Accuracy: 64.35643564356435%, Average Validation Loss: 4.910674285888672\n",
      "Epoch 59:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411965698003769\n",
      "Validation: \n",
      " Validation Accuracy: 66.996699669967%, Average Validation Loss: 4.8757343769073485\n",
      "Epoch 60:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4118665754795074\n",
      "Validation: \n",
      " Validation Accuracy: 68.97689768976898%, Average Validation Loss: 4.836893463134766\n",
      "Epoch 61:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412795752286911\n",
      "Validation: \n",
      " Validation Accuracy: 66.00660066006601%, Average Validation Loss: 4.902812433242798\n",
      "Epoch 62:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411612182855606\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.858863878250122\n",
      "Epoch 63:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411695778369904\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.8468252658844\n",
      "Epoch 64:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411899775266647\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.874839782714844\n",
      "Epoch 65:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.41155943274498\n",
      "Validation: \n",
      " Validation Accuracy: 69.63696369636963%, Average Validation Loss: 4.85220193862915\n",
      "Epoch 66:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411379635334015\n",
      "Validation: \n",
      " Validation Accuracy: 68.97689768976898%, Average Validation Loss: 4.84026780128479\n",
      "Epoch 67:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411541938781738\n",
      "Validation: \n",
      " Validation Accuracy: 68.97689768976898%, Average Validation Loss: 4.846285057067871\n",
      "Epoch 68:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4116431176662445\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.867368364334107\n",
      "Epoch 69:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411506146192551\n",
      "Validation: \n",
      " Validation Accuracy: 69.96699669966996%, Average Validation Loss: 4.840494775772095\n",
      "Epoch 70:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411422401666641\n",
      "Validation: \n",
      " Validation Accuracy: 67.65676567656766%, Average Validation Loss: 4.851000070571899\n",
      "Epoch 71:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411621034145355\n",
      "Validation: \n",
      " Validation Accuracy: 69.96699669966996%, Average Validation Loss: 4.834549617767334\n",
      "Epoch 72:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411174833774567\n",
      "Validation: \n",
      " Validation Accuracy: 66.66666666666667%, Average Validation Loss: 4.858924102783203\n",
      "Epoch 73:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4115952253341675\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.871720314025879\n",
      "Epoch 74:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411419242620468\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.837034463882446\n",
      "Epoch 75:\n",
      "Training: \n",
      " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4109832644462585\n",
      "Validation: \n",
      " Validation Accuracy: 68.97689768976898%, Average Validation Loss: 4.846198797225952\n",
      "Epoch 76:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409587770700455\n",
      "Validation: \n",
      " Validation Accuracy: 66.33663366336634%, Average Validation Loss: 4.871611404418945\n",
      "Epoch 77:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409377098083496\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.826735019683838\n",
      "Epoch 78:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409126460552216\n",
      "Validation: \n",
      " Validation Accuracy: 69.3069306930693%, Average Validation Loss: 4.836365413665772\n",
      "Epoch 79:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409206330776215\n",
      "Validation: \n",
      " Validation Accuracy: 67.65676567656766%, Average Validation Loss: 4.8399741649627686\n",
      "Epoch 80:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409336686134338\n",
      "Validation: \n",
      " Validation Accuracy: 65.67656765676567%, Average Validation Loss: 4.87538251876831\n",
      "Epoch 81:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409109145402908\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.835529136657715\n",
      "Epoch 82:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409252315759659\n",
      "Validation: \n",
      " Validation Accuracy: 67.65676567656766%, Average Validation Loss: 4.853705596923828\n",
      "Epoch 83:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408854991197586\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.832612371444702\n",
      "Epoch 84:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408980518579483\n",
      "Validation: \n",
      " Validation Accuracy: 69.63696369636963%, Average Validation Loss: 4.830557489395142\n",
      "Epoch 85:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409017652273178\n",
      "Validation: \n",
      " Validation Accuracy: 66.00660066006601%, Average Validation Loss: 4.850003623962403\n",
      "Epoch 86:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409248203039169\n",
      "Validation: \n",
      " Validation Accuracy: 66.00660066006601%, Average Validation Loss: 4.8609850883483885\n",
      "Epoch 87:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.4088966846466064\n",
      "Validation: \n",
      " Validation Accuracy: 68.97689768976898%, Average Validation Loss: 4.8375585079193115\n",
      "Epoch 88:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409011632204056\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.839452648162842\n",
      "Epoch 89:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.4090026915073395\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.839881896972656\n",
      "Epoch 90:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408896595239639\n",
      "Validation: \n",
      " Validation Accuracy: 69.3069306930693%, Average Validation Loss: 4.838299036026001\n",
      "Epoch 91:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.4087163507938385\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.83735408782959\n",
      "Epoch 92:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408800393342972\n",
      "Validation: \n",
      " Validation Accuracy: 69.3069306930693%, Average Validation Loss: 4.8345513343811035\n",
      "Epoch 93:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408697426319122\n",
      "Validation: \n",
      " Validation Accuracy: 68.97689768976898%, Average Validation Loss: 4.824241209030151\n",
      "Epoch 94:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408783167600632\n",
      "Validation: \n",
      " Validation Accuracy: 68.64686468646865%, Average Validation Loss: 4.825793313980102\n",
      "Epoch 95:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408752232789993\n",
      "Validation: \n",
      " Validation Accuracy: 68.31683168316832%, Average Validation Loss: 4.841542768478393\n",
      "Epoch 96:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408666253089905\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.830492830276489\n",
      "Epoch 97:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408630073070526\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.836542844772339\n",
      "Epoch 98:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408743351697922\n",
      "Validation: \n",
      " Validation Accuracy: 68.97689768976898%, Average Validation Loss: 4.82788233757019\n",
      "Epoch 99:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408624678850174\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.8308628559112545\n",
      "Epoch 100:\n",
      "Training: \n",
      " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408637195825577\n",
      "Validation: \n",
      " Validation Accuracy: 67.98679867986799%, Average Validation Loss: 4.8244383335113525\n"
     ]
    }
   ],
   "source": [
    "epoch_number = 0\n",
    "EPOCHS = 100\n",
    "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-4)\n",
    "for epoch in range (EPOCHS):\n",
    "    print(\"Epoch {}:\".format(epoch_number+1))\n",
    "    train_loss, valid_loss = training_one_epoch()\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
